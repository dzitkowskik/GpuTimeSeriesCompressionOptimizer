\documentclass[12pt, twoside, openany]{report}
\usepackage[dvips]{graphicx,color,rotating}
\usepackage[cp1250]{inputenc}
\usepackage{t1enc}
\usepackage{a4wide}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage[MeX]{polski}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{left=25mm,right=25mm,%
bindingoffset=10mm, top=25mm, bottom=25mm}
\usepackage{amssymb, latexsym}
\usepackage{amsthm}
\usepackage{palatino}
\usepackage{array}
\usepackage{pstricks}
\usepackage{textcomp}
\theoremstyle{definition}
\newtheorem{theorem}{Twierdzenie}[section]
\newtheorem{remark}{Uwaga}[section]
\newtheorem{definition}{Definicja}[section]
\newtheorem{alg}{Algorytm}[chapter]
\newtheorem{prz}{Przypadek}[section]
\newtheorem{np}{Przyk³ad}[section]
\newtheorem{lemma}[theorem]{Lemat}
\linespread{1.5}
\newcommand*{\norm}[1]{\left\Vert{#1}\right\Vert}
\newcommand*{\abs}[1]{\left\vert{#1}\right\vert}
\newcommand*{\om}{\omega}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{listings}

\author{Karol Dzitkowski}
\title{Optymalizator kompresji szeregów czasowych na GPU}
\begin{document}
\begin{titlepage}
\pagestyle{empty}

\noindent
\begin{Large}
\begin{table}[t]
\centering
\begin{tabular}[t]{lcr}
 \includegraphics[width=70pt,height=70pt]{PW} & POLITECHNIKA WARSZAWSKA & \includegraphics[width=70pt,height=70pt]{MiNI}\\
& WYDZIA£ MATEMATYKI & \\
& I NAUK INFORMACYJNYCH &
\end{tabular}
\end{table}

% \vfill
\begin{center}PRACA DYPLOMOWA MAGISTERSKA\end{center}
\begin{center}INFORMATYKA\end{center}\end{Large}
% \vfill
\begin{center}
\Huge
\textbf{Optymalizator kompresji szeregów czasowych na GPU}
\end{center}
% \vfill\vfill
\vfill
\begin{center}
\Large
Autor:\\
\LARGE
Karol Dzitkowski
\end{center}
\vfill
\begin{center}
\Large
Promotor: dr in¿. Krzysztof Kaczmarski
\end{center}
\vfill
\begin{center}
\large
Warszawa, luty 2016
\end{center}
\newpage
\hfill
\begin{table}[b]
\centering
\begin{tabular}[t]{ccc}
............................................. & \hspace*{100pt} & .............................................\\
podpis promotora & \hspace*{100pt} & podpis autora
\end{tabular}
\end{table}


% \maketitle
\end{titlepage}
\thispagestyle{empty}
\newpage
\pagestyle{headings}
\setcounter{page}{1}
\hyphenation{Syl-ves-tra}
\hyphenation{Syl-ves-ter-a}

\renewcommand{\abstractname}{Abstrakt}

\begin{abstract}
Wiele urz¹dzeñ takich jak czujniki, stacje pomiarowe, czy nawet serwery, produkuj¹ ogromne iloœci danych w postaci szeregów czasowych, które nastêpnie s¹ przetwarzane i sk³adowane do póŸniejszej analizy. Ogromn¹ rolê w tym procesie stanowi przetwarzanie danych na kartach graficznych w celu przyspieszenia obliczeñ. Aby wydajnie korzystaæ z GPGPU przedstawiono szereg rozwi¹zañ, korzystaj¹cych z kart graficznych jako koprocesory w bazach danych lub nawet bazy danych po stronie GPU. We wszystkich rozwi¹zaniach bardzo istotn¹ rolê stanowi kompresja danych. Szeregi czasowe s¹ bardzo szczególnym rodzajem danych, dla których kluczowy jest dobór odpowiedniej kompresji wedle charakterystyki danych szeregu. W tej pracy przedstawiê nowe podejœcie do kompresji szeregów czasowych po stronie GPU, przy u¿yciu planera buduj¹cego na bie¿¹co drzewa kompresji na podstawie statystyk nap³ywaj¹cych danych. Przedstawione rozwi¹zanie kompresuje dane za pomoc¹ lekkich i bezstratnych kompresji w technologii CUDA.
\end{abstract}

\renewcommand{\abstractname}{Abstract}

\begin{abstract}
Many devices such as sensors, measuring stations or even servers produce enormous amounts of data in the form of time series, which are then processed and stored for later analysis. A huge role in this process takes data processing on graphics cards in order to accelerate calculations. To efficiently use the GPGPU a number of solutions has been presented, that use the GPU as a coprocessor in a databases. There were also attempts to create a GPU-side databases. It has been known that data compression plays here the crucial role. Time series are special kind of data, for which choosing the right compression according to the characteristics of the data series is essencial. In this paper I present a new approach to compression of time series on the side of the GPU, using a planner to keep building the compression tree based on statistics of incoming data. The solution compresses data using lightweight and lossless compression in CUDA technology.
\end{abstract}

%-----------Pocz¹tek czêœci zasadniczej-----------

\chapter{Wstêp}
Poni¿sza praca zawiera opis implementacji optymalizatora kompresji szeregów czasowych, bazuj¹cego na dynamicznie generowanych statystykach danych. Pomys³ opiera siê na tworzeniu drzew kompresji (kompresja kaskadowa) oraz zbieraniu statystyk o krawêdziach takich drzew - jak dobrze dana para kompresji sprawdza siê dla nap³ywaj¹cych danych. System bêdzie równie¿ dynamicznie zmienia³ - korygowa³, takie drzewa w zale¿noœci od charakterystyki kolejnych paczek danych.
W za³o¿eniu system ma umo¿liwiæ kompresjê du¿ych iloœci danych przy wykorzystaniu potencja³u obliczeniowego wspó³czesnych kart graficznych. 

\section{Procesory graficzne}
Procesory graficzne sta³y siê znacz¹cymi i potê¿nymi koprocesorami obliczeñ dla wielu aplikacji i systemów, takich jak bazy danych, badania naukowe czy wyszukiwarki www. Nowoczesne GPU posiadaj¹ moc obliczeniow¹ o rz¹d wiêksz¹ ni¿ zwykle, wielordzeniowe procesory CPU, takie jak AMD FX 8XXX czy Intel Core i7. Dla przyk³adu flagowa konstrukcja firmy NVIDIA - GeForce GTX Titan X osi¹ga moc 6600 GFLOPS (miliardów operacji zmiennoprzecinkowych na sekundê), przy $336 GB/s$ przepustowoœci pamiêci, podczas gdy najszybsze procesory takie jak Intel Core i7-5960x osi¹gaj¹ nieca³e 180 GFLOPS, przy przepustowoœci $68 GB/s$. Kartom graficznym dorównuj¹ tylko inne jednostki typu SIMD, na przyk³¹d karty obliczeniowe Xeon Phi. Pomimo tak osza³amiaj¹cych wyników, programowanie na jednostkach SIMD jest o wiele trudniejsze,   
jak równie¿ ograniczone przepustowoœci¹ szyny PCI-E, która wynosi w porywach $8 GB/s$, co dodatkowo przemawia za u¿yciem kompresji przy przetwarzaniu szeregów czasowych, choæby w celu przyspieszenia kopiowania danych z i na kartê graficzn¹ w celu wykonania obliczeñ.

\section{Szeregi czasowe}

	\begin{figure}[h!]
		\centering
		\hspace*{-0.5cm}\includegraphics[scale=0.55]{img/TS}
		\caption{Szereg czasowy}
		\label{ref:ts}
	\end{figure}

Terabajty danych w postaci szeregów czasowych s¹ przetwarzane i analizowane ka¿dego dnia na ca³ym œwiecie. Zapytania i agregacje na tak wielkich porcjach danych jest czasoch³onne i wymaga du¿ej iloœci zasobów. Aby zmierzyæ siê z tym problemem, powsta³y wyspecjalizowane bazy danych, wspieraj¹ce analizê szeregów czasowych. Wa¿nym czynnikiem w tych rozwi¹zaniach jest kompresja oraz u¿ycie procesorów graficznych w celu przyspieszenia obliczeñ. Aby przetwarzaæ dane na GPU bez koniecznoœci ich ci¹g³ego kopiowania poprzez szynê PCI-E, powstaj¹ bazy danych po stronie GPU (najczêœciej rozproszone), takie jak MapD lub DDJ (zaproponowana miêdzy innymi przeze mnie w poprzedniej pracy - in¿ynierskiej).
Innymi rozwi¹zaniami s¹ koprocesory obliczeniowe GPU, wspomagaj¹ce dzia³anie baz takich jak Cassandra, HBase, TepoDB, OpenTSDB czy PostgreSQL. Charakterystyka danych wielu szeregów wskazuje, ¿e przy odpowiedniej obróbce mog¹ byæ kompresowane z bardzo du¿ym wspó³czynnikiem, szczególnie jeœli by³oby mo¿liwe kompresowanie za pomoc¹ dynamicznie zmieniaj¹cych siê ci¹gów (ró¿nych) algorytmów kompresji i transformacji danych. Dla przyk³adu, jeœli jakiœ fragment szeregu jest sta³y, z nielicznymi wyj¹tkami, warto by³oby usun¹æ wyj¹tki, a resztê skompresowaæ jako jedn¹ liczbê - uzyskuj¹c wspó³czynnik kompresji rzêdu d³ugoœci danych.   

\section{SIMD i lekka kompresja}
Bazy danych przechowuj¹ce szeregi czasowe s¹ najczêœciej zorientowane kolumnowo oraz stosuj¹ metody lekkiej kompresji w celu oszczêdnoœci pamiêci. W tych przypadkach stosuje siê metody lekkiej kompresji, takie jak kodowanie s³ownikowe, delta lub sta³ej liczby bitów, zamiast bardziej skomplikowanych i wolniejszych metod, które czêsto zapewni³yby lepszy poziom kompresji. Systemy te ³aduj¹ swoje dane do pamiêci trwa³ej paczkami, które mog¹ byæ kompresowane osobo i byæ mo¿e przy u¿yciu ró¿nych algorytmów, zmieniaj¹cych siê dynamicznie w czasie. Takie kolumny wartoœci numerycznych tego samego typu wspaniale przetwarza siê przy u¿yciu procesorów typu SIMD, co daje wielokrotne przyspieszenie w stosunku do tradycyjnych architektur. Okazuje siê ¿e wiêkszoœæ algorytmów lekkiej kompresji mo¿e z du¿ym powodzeniem byæ w ten sposób zrównoleglona. Równie¿ dynamiczne generowanie statystyk nap³ywaj¹cych danych mo¿e byæ przyspieszone z u¿yciem SIMD, co otwiera mo¿liwoœæ implementacji wydajnych systemów, dynamicznie optymalizuj¹cych u¿yte kompresje w celu zwiêkszenia wspó³czynnika kompresji danych. Dodatkowo u¿ycie kaskadowej kompresji mo¿e wielokrotnie wzmocniæ poziom kompresji, pod warunkiem stworzenia dobrego planu kompresji, w³aœnie na podstawie wygenerowanych statystyk. Ogromna moc obliczeniowa procesorów graficznych mo¿e pozwoliæ wygenerowaæ taki plan w rozs¹dnym czasie. Takie u¿ycie jest mo¿liwe np. w bazach danych po stronie GPU, gdzie jest to niezmiernie wa¿ne z powodu œcis³ego limitu pamiêci na kartach i ich wysokiego kosztu.

\section{Zawartoœæ pracy}
W tej pracy przedstawiê planer kompresji (optymalizator) kompresuj¹cy nap³ywaj¹ce paczki danych. Zaprezentujê nowe podejœcie, planera buduj¹cego drzewa kompresji i ucz¹cego siê ich konstrukcji na podstawie na bie¿¹co generowanych statystyk wêz³ów takich drzew (jak równie¿ statystyk nap³ywaj¹cych danych). Przedstawiê równie¿ zaimplementowane œrodowisko oraz u¿yte algorytmy lekkiej kompresji. W ramach tej pracy stworzone zosta³y 4 biblioteki, wykorzystuj¹ce technologiê NVIDIA CUDA, tworz¹ce framework optymalizatora kompresji oraz program w sposób równoleg³y kompresuj¹cy kolumny podanego szeregu czasowego.
Nastêpny podrozdzia³ zawiera opis wczeœniejszych prac prowadzonych w tych tematach, a tak¿e krótki opis architektury CUDA. W rozdziale 2 omówiê stworzony framework oraz metody lekkiej kompresji, ze szczególnym uwzglêdnieniem kompresji FL oraz GFC. Rozdzia³ 3 jest w ca³oœci poœwiêcony optymalizatorowi kompresji oraz generowaniu drzew i statystyk. Nastêpnie przedstawiê wyniki prac i eksperymentów. Ostatni rozdzia³ (pi¹ty) to podsumowanie oraz zakres przysz³ych prac i optymalizacji.

\section{Powi¹zane prace}

\subsection{Szeregi czasowe}
Szeregi czasowe s¹ typem danych dla których istnieje wiele efektywnych sposobów kompresji zale¿nych od ich charakterystyki. W wielu pracach przedstawiono podejœcia do tego problemu od strony lekkiej kompresji. Najczêstszymi z nich s¹ kodowanie ekstremami \cite{FINK}, sta³ej d³ugoœci bitów \cite{PRZYMUS1}, czyli tzw. NULL Suppression (NS) oraz proste kodowania s³ownikowe np. wszystkich unikalnych wartoœci, które mo¿na zakodowaæ pewn¹ za³o¿on¹ z góry liczb¹ bitów \cite{ABADI}. \\
Dodatkowo do kompresji szeregów stosuje siê metody regresji \cite{MENSMANN}. Autor stosuje Piecewise Regression - regresjê odcinkow¹, polegaj¹c¹ na przybli¿aniu kawa³ków szeregu funkcj¹, np. wielomianem. Ma to swoj¹ wersjê stratn¹ jak i bezstratn¹, gdzie mo¿emy zapisaæ ró¿nicê od zadanej funkcji i wynik zapisaæ na mniejszej liczbie bitów. \\ Szeregi czasowe to nie tylko liczby ca³kowitoliczbowe. Analizuje siê tak¿e wiele sposobów kompresji liczb zmiennoprzecinkowych pojedynczej i podwójnej precyzji. Najczêœciej próbuje siê zamieniæ liczbê u³amkow¹ na ca³kowit¹ stosuj¹c skalowanie \cite{PRZYMUS2}. Istniej¹ te¿ bardziej skomplikowane metody na przyk³ad kompresji liczb double algorytmem FPC \cite{BURTSCHER}, który kompresuje liniow¹ sekwencjê liczb o podwójnej precyzji (IEEE 754), sekwencyjnie przewiduj¹c ka¿d¹ wartoœæ, a nastêpnie wykonuj¹c operacjê XOR z prawdziw¹ wartoœci¹ szeregu, po czym usuwane s¹ wiod¹ce zera.

\subsection{SIMD SSE}
Bior¹c pod uwagê algorytmy lekkiej kompresji dla szeregów, warto zwróciæ uwagê na udane próby optymalizacji z u¿yciem prostego SIMD jakim s¹ operacje wektorowe SSE na procesorach Intel \cite{ZHAO} \cite{LEMIRE}. W tych pracach pokazano przek³ad algorytmów kodowania z wyrównaniem do bajtów (Byte-Aligned Coding) oraz do s³ów (Word-Aligned Coding) oraz zmierzono wydajnoœæ implementacji wektorowej wersji tych kodowañ z u¿yciem SSE. Autorzy zastosowali równie¿ binarne pakowanie (Binary Packing) w formie algorytmu FOR (Frame of Reference) \cite{GOLDSTEIN} dziel¹c dane na bloki o d³ugoœci 128 elementów (zmiennych ca³kowitych o d³ugoœci 32 bitów) i stosuj¹c patchowanie. Taki algorytm okaza³ siê najwydajniejszy. Pokazano, ¿e bez spadku jakoœci kompresji mo¿na uzyskaæ w ten sposób wzrost szybkoœci kompresji od 2 do 4 razy w stosunku to tradycyjnej implementacji.

\subsection{Obliczenia GPU}
Dziêki ogromnej mocy obliczeniowej kart graficznych uzyskano znacz¹cy wzrost wydajnoœci wielu algorytmów daj¹cych siê w mniejszym lub wiêkszym stopniu zrównolegliæ. Przyk³adowymi algorytmami o tej w³aœciwoœci s¹ choæby radix sort\cite{SORT}, hashowanie kuku³cze\cite{PHASH,CHASH}, sumy prefixowe\cite{PPS} i inne zaimplementowane w podstawowych bibliotekach takich jak 
CUDPP\footnote{CUDA Data Parallel Primitives - http://cudpp.github.io/} 
czy 
Thrust\footnote{Parallel algorithms library - https://developer.nvidia.com/thrust}. \\
Najwa¿niejsze s¹ jednak bardzo zadowalaj¹ce rezultaty zrównoleglania algorytmów u¿ywanych w bazach danych takich jak index search\cite{ANH}, wszelkiego rodzaju agregacje i operacje join, scatter i gather\cite{RUSSEK} oraz obliczanie statystyk danych\cite{KACZMAR1} jak równie¿ dopasowywanie wyra¿eñ regularnych\cite{MORISHIMA}. Dla przyk³adu wzrost wydajnoœci oferowany przez algorytmy z biblioteki Thrust, która jest niejako odpowiednikiem Std, jest œrednio 10-krotny\cite{CUDA_PERF} w stosunku do najszybszych wersji CPU. Wiêkszoœæ przytoczonych wy¿ej przyk³adów równie¿ reprezentuje wzrost wydajnoœci o rz¹d wielkoœci. W pracach odnoœnie akceleracji baz danych za pomoc¹ technologii CUDA, autorzy otrzymuj¹ przyspieszenie $20-60$ krotne\cite{BAKKUM}, w przypadku operacji \emph{SELECT WHERE} i \emph{SELECT JOIN} z agregacjami\cite{RUSSEK}. Jednak jest to liczone bez uwzglêdniania czasu kopiowania danych na GPU. Jak obliczono zajmuje to œrednio ok $90\%$ czasu dzia³ania algorytmów\cite{KACZMAR2}.

\subsection{Kompresje}
Opisane wy¿ej algorytmy lekkiej kompresji szeregów czasowych, w wiêkszoœci zosta³y zaimplementowane przez Fang et al.\cite{FANG} oraz Przymus et al.\cite{KACZMAR2} w ich pracach, gdzie autorzy zaznaczaj¹ ogromny wzrost przepustowoœci takich algorytmów oraz ich wysok¹ skutecznoœæ w kompresji szeregów. W przypadku pierwszego jest to nawet to 56 GB/s dekodowania, a dla drugiego od 2 do 40 GB/s kodowania w zale¿noœci od stopnia skomplikowania algorytmu. Wiêkszoœæ przewidzianych metod ma swoje odpowiedniki z patchowaniem, w którym elementy niepasuj¹ce odk³adane s¹ do osobnej tablicy wyj¹tków. Prace te odnosz¹ siê jednak tylko do liczb ca³kowitych, a najczêœciej ca³kowitych bez znaku (naturalnych). Lekk¹ kompresjê liczb zmiennopozycyjnych o podwójnej precyzji przestawi³ w swojej pracy O'Neil et al.\cite{ONEIL}, w której stosuj¹c technologiê CUDA i dziel¹c dane na odpowiednie bloki, zastosowano wariacjê algorytmu FOR i osi¹gniêto bardzo dobre wyniki rzêdu 75 Gb/s kodowania oraz a¿ 90 Gb/s dekodowania (daje to podobne rezultaty jak implementacja algorytmu FL dla liczb naturanlych\cite{PRZYMUS1}). Algorytm nazwano GFC i jest uogólnienie równie¿ dla liczb o pojedynczej precyzji przedstawiê dok³adnie w kolejnym rozdziale. \\
Wzrost wydajnoœci osi¹gniêto równie¿ na tle bardziej skomplikowanych metod (daj¹cych czêsto lepsze wspó³czynniki kompresji) jak kodowanie Huffmana\cite{CURRY}, gdzie równoleg³a implementacja na GPU uzyska³a 2 do 5 krotne przyspieszenie, natomiast przepustowoœæ takiej kompresji to dla porównania 300 do 500 MB/s. Próby zoptymalizowania algorytmów takich jak LZSS w pracach Ozsoy et al.\cite{OZSOY}, skoñczy³y siê lekkim (max 2.2x) wzrostem wydajnoœci (w stosunku do wielordzeniowych implementacji CPU), przy uzyskanej przepustowoœci 1700 Mb/s w konfiguracji z dwoma kartami GPU\cite{OZSOY3}. Równie¿ inni autorzy maj¹ nadziejê na optymalizacje z u¿yciem kodowañ s³ownikowych takich jak LZW pobij¹ wyniki CPU jeœli dobrze przepisze siê je do architektury SIMD\cite{SHYNI}. Mo¿na dodatkowo spotkaæ wariacje algorytmu LZSS przepisanego na CUDA, takie jak CANLZSS\cite{DTU}, która wed³ug autora przewy¿sza wydajnoœci¹ ponad 60 razy seryjn¹ implementacjê zwyk³ego LZSS. Kolejna optymalizacja GLZSS, w której zreorganizowano s³ownik to postaci tablicy haszy, oraz przyspieszono porównywanie podci¹gów (substrings) równie¿ zrównoleglaj¹c je na GPU, osi¹gaj¹c dwukrotne przyspieszenie wzglêdem poprzednich prac\cite{ZU}. \\
Porównuj¹c szybkoœæ dzia³ania oraz wspó³czynniki kompresji uzyskane przez autorów, mo¿na dojœæ do wniosku ¿e zastosowanie metod lekkich kompresji dla szeregów czasowych, w miejscach gdzie szczególnie wa¿na jest szybka dekompresja (bazy danych), jest uzasadnione. 

\subsection{Planery kompresji}
Okazuje siê, ¿e aby wielokrotnie zwiêkszyæ wspó³czynnik kompresji szeregu czasowego warto go przetransformowaæ przed kompresj¹ b¹dŸ nawet skompresowaæ wielokrotnie ró¿nymi algorytmami. W tym celu powsta³y planery kompresji które dzia³aj¹ na zasadzie kodowania kaskadowego, czyli ci¹gu nastêpuj¹cych po sobie kodowañ, tworz¹cych drzewo. Dziêki zastosowaniu procesorów graficznych osi¹gniêto bardzo dobre wyniki zarówno pod wzglêdem wspó³czynnika kompresji, jak i szybkoœci dzia³ania. Dla przyk³adu przepustowoœæ kodowania 45 GB/s oraz dekodowania 56 GB/s zosta³a osi¹gniêta przez Fang et al.\cite{FANG}. W tym przypadku pos³uguj¹c siê heurystykami wykorzystuj¹cymi statystyki danych wejœciowych, spoœród ogromnej iloœci dostêpnych schematów (> 500 tys.) kodowania (planów), wybierano najlepiej pasuj¹ce (np. dla danych posortowanych powinny zaczynaæ siê od RLE itd.). Nastêpnie wybierano spoœród nich plan spe³niaj¹cy zdefiniowane normy, np. plan o najwiêkszym wspó³czynniku kompresji. Statystyki na których oparty by³ algorytm brane by³y z informacji o kolumnie w bazie danych. Zanotowano du¿o lepsz¹ kompresjê ni¿ w przypadku tradycyjnego kodowania pojedyncz¹ metod¹, dla realistycznych danych. \\
Kolejnym, podobnym podejœciem do planera jest praca Przymusa et al.\cite{PRZYMUS2}, w której plan z³o¿ony jest z trzech warstw metod nastêpuj¹cych po sobie: transformacji, kodowania bazowego i pomocniczego. Cech¹ charakterystyczn¹ tego rozwi¹zania jest dynamiczny generator statystyk, który uaktualnia statystyki w momencie tworzenia planu, wykorzystuj¹c w³aœciwoœci poszczególnych metod takiej kompresji (szczególnie w przypadku minimalnej iloœci bitów potrzebnych do zapisania ka¿dej liczby z danego wektora danych). Dodatkowo praca ta implementuje znajdowanie optimum ze wzglêdu na dwie sprzeczne zmienne (bi-objective optimizer): szybkoœæ dzia³ania i jakoœæ kompresji, stosuj¹c optymalnoœæ Pareto\cite{PARETO}. Generowanie statystyk dla takiego planera na GPU zapewnia do 70 razy lepsz¹ wydajnoœæ w stosunku do analogicznej implementacji na CPU\cite{KACZMAR2}.

\section{CUDA}

	\begin{figure}[h!]
		\centering
		\hspace*{-0.5cm}\includegraphics[scale=0.55]{img/CUDA_arch}
		\caption{Architektura NVIDIA CUDA}
		\label{ref:cuda_arch}
	\end{figure}

Jedn¹ z wielu zalet architektury kart graficznych jest to, ¿e sk³adaj¹ siê z kilku multiprocesorów (SMs - Streaming Multiprocessors) architektury SIMD. Jest to w³aœciwie architektura typu SIMT, gdzie multiprocesor wykonuje w¹tki w grupach o licznoœci 32 zwanymi \emph{warps}. Architektura ta jest zbli¿ona do SIMD z t¹ ró¿nic¹, ¿e to nie organizacja wektora danych kontroluje jednostki obliczeniowe, a organizacja instrukcji pojedyñczego w¹tku. Umo¿liwia on zatem pisanie równolegle wykonywanego kodu dla niezale¿nych i skalowalnych w¹tków, jak i dla w¹tkuch koordynowanych danymi.
Wszystkie w¹tki wykonuj¹ ten sam kod funkcji kernela. Ponadto CUDA tworzy abstrakcjê bloków w¹tków, które zorganizowane s¹ w siatkê (GRID) i wspó³dziel¹ zasoby multiprocesora. Wa¿na jest równie¿ hierarchia pamiêci, w której czêœæ przydzielana jest w¹tkom w postaci pamiêci lokalnej i rejestrów, oraz pamiêci wspó³dzielonej przez w¹tki z tego samego bloku (Shared Memory) - te pamiêci musz¹ byæ znane w trakcie kompilacji kernela. Najwolniejsza jest pamiêæ globalna (Device Memory), wspólna dla wszystkich w¹tków, wszystkich bloków, na wszystkich multiprocesorach. Dok³adny opis tej architektury mo¿na znaleŸæ bezpoœrednio na stronie producenta.

	\begin{figure}[h!]
		\centering
		\hspace*{-0.5cm}\includegraphics[scale=0.55]{img/CUDA_grid}
		\caption{Abstrakcja bloków i siatki w CUDA}
		\label{ref:cuda_grid}
	\end{figure}

\chapter{Opis systemu}

\section{Kodowania}

Przedstawiê krótko kodowania u¿yte w implementacji planera kompresji oraz ich modyfikacje, a nastêpnie opiszê szczegó³owo dwa najwa¿niejsze, które zwykle koñcz¹ œcie¿ki kompresji w generowanych planach. S¹ to kodowania bazuj¹ce na pomyœle usuwania zbêdnych zer wiod¹cych, dla liczb naturalnych - AFL oraz u³amkowych - GFC. Trzeba zauwa¿yæ, ¿e operacja patchowania jest tutaj zaimplementowana odmiennie ni¿ w innych publikacjach, jako osobny rodzaj kodowania, mog¹cy przybieraæ wiele form, natomiast algorytmy nie maj¹ swoich wersji z patchowaniem.\\

\subsection{Podstawowe algorytmy transformacji szeregów}
\begin{description}
\item[Delta] -- Zapisuje zmiany pomiêdzy kolejnymi danymi w szeregu, zapisuj¹c pierwsz¹ wartoœæ w metadanych. Bardzo dobrze sprawdza siê na danych posortowanych b¹dŸ o zmianach liniowych o sta³ej wartoœci. Algorytm ten zosta³ zaimplementowany czêœciowo z u¿yciem biblioteki Thrust i funkcji \emph{inclusive\_scan} - do dekodowania.
\item[Scale] Bardzo prosta transformacja, odmienna od wielu implementacji o tej samej nazwie, polegaj¹ca na odjêciu lub dodaniu (dla liczb ujemnych) najmniejszej dodatniej wartoœci szeregu. Bardzo dobrze sprawdza siê dla du¿ych wartoœci szeregu o ma³ej wariancji. Dla przyk³adu maj¹c wartoœci ${1234000, ..., 1234500, ..., 1234999}$ dane zostan¹ zapisane jako ${0, 1, ..., 999}$ i bêd¹ mog³y byæ zapisane na du¿o mniejszej liczbie bitów.
\item[FloatToInt] -- To jest wersja algorytmu któr¹ czêsto w literaturze nazywa siê mianem scale. Znaj¹c maksymaln¹ precyzjê danych zmiennopozycyjnych, mo¿na zapisaæ te dane jako liczby ca³kowite, mno¿¹c przez odpowiedni¹ potêgê liczby 10. Tak wiêc maj¹c wektor cen w z³otówkach, mo¿na przemno¿yæ cenê 99.99 przez 100 otrzymuj¹c 9999 i zmieniæ reprezentacjê liczb na ca³kowite. Poniewa¿ reprezentacja liczb ca³kowitych mo¿e daæ siê lepiej skompresowaæ.
\item[Patch] -- Bardzo wa¿nym zadaniem w kompresji szeregów czasowych jest usuwanie wartoœci odstaj¹cych tzw. \emph{outliers}. Kodowanie to dzieli wektor danych na dwa wektory wzglêdem zdefiniowanego operatora, mówi¹cego np. ¿e jako wartoœci odstaj¹ce nale¿y uznaæ wszystkie wartoœci przekraczaj¹ce $90\%$ wartoœci maksymalnej. W ten sposób wiele ró¿nych wersji patchowania mo¿e byæ zdefiniowanych, mo¿e na przyk³ad dzieliæ wartoœci na ujemne i dodatnie. 

	\begin{figure}[h!]
		\centering
		\includegraphics{img/Patch}
		\caption{Przyk³ad u¿ycia patchowania z operatorem - "wiêkszy od zera"}
		\label{ref:patch}
	\end{figure}

W przeciwieñstwie do innych prac algorytm ten nie zapisuje wyj¹tków wraz z ich pozycjami, i umo¿liwia w ten sposób lepsze ich skompresowanie w dalszych krokach. Zamiast tego trzyma w metadanych zapisany bitowo stempel przynale¿noœci poszczególnych elementów wektora to pierwszej lub drugiej tablicy wynikowej. Rozmiar takich metadanych wynosi $(N+32)$ bitów, gdzie $N$ to liczba kompresowanych elementów.
\end{description}
\newpage

\subsection{Global memory coalescing}


Tablice zaalokowane w pamiêci urz¹dzenia GPU Nvidia s¹ wyrównane do bloków o wielkoœci 256 bajtów przez sterownik urz¹dzenia. Urz¹dzenie mo¿e dostaæ siê do pamiêci przez 32, 64 lub 128 bajtowe transakcje które s¹ wyrównane do ich wielkoœci. Odczytuj¹c lub zapisuj¹c do pamiêci globalnej, wa¿ne jest zatem aby w¹tki wymaga³y dostêpu zawsze do kolejnych rekordów tablicy, najlepiej wszystkie nale¿¹ce do tego samego \emph{warp}. Aby tak siê sta³o mo¿na zastosowaæ poni¿szy algorytm obliczania indeksów wejœciowych dla danego w¹tku CUDA. \\
Oznaczmy jako:
\begin{itemize}
	\item $\Sigma$ -- iloœæ przetwarzanych elementów
	\item $\omega$ -- iloœæ w¹tków w grupie
	\item $\kappa$ -- iloœæ elementów w bloku danych
	\item $\lambda$ -- iloœæ elementów przetwarzanych przez pojedynczy w¹tek
	\item $B_{size}$ -- wielkoœæ bloków w¹tków -- (CUDA block size)
	\item $B_{count}$ -- iloœæ bloków -- (CUDA block count)
	\item $w_{g}$ -- iloœæ grup w bloku
	\item $W_{lane}(t) = t_{idx} (mod\ \omega)$ -- indeks w¹tku t w grupie -- (warp lane)
\end{itemize}
\noindent
Za³ó¿my ¿e chcemy przetworzyæ 1MB danych tj. $1024*1024$ elementy, u¿ywaj¹c bloków z 4 grupami po 32 w¹tki ka¿dy, czyli 4 warpy. Ponadto chcemy aby bloki danych mia³y 32 rekordy, a ka¿dy w¹tek przetwarza³ 16 elementów: $\Sigma = 1024*1024$, $\omega = 32$,  $\kappa = 32$, $\lambda = 16$, $w_{g}$ = 4. \\
Bardzo ³atwo mo¿na obliczyæ ile wynosi rozmiar pojedynczego bloku $B_{size}$, oraz iloœæ wszystkich bloków, potrzebnych do przetworzenia wszystkich elementów wektora wejœciowego $B_{count}$:
$$B_{size} = \omega * w_{g} $$,
$$ B_{count} = \frac{\Sigma + B_{size}*\lambda - 1}{B_{size}*\lambda} $$

Przyjmuj¹c pocz¹tkowy indeks bloków danych dla w¹tku $t$ o indeksie $t_{idx}(t)$ z bloku $b_{idx}(t)$ za $\Psi(t) = (t_{idx}(t) - W_{lane}(t)) * \lambda / 32$, indeks bloku danych dla danego w¹tku mo¿na obliczyæ za pomoc¹ wzoru:
$$ d_{b}(t) = b_{idx}(t) * w_{g} * \lambda + \Psi(t) $$
co dla naszego przyk³adu obrazuje rysunek \ref{ref:cuda_data_blocks}.
\begin{figure}[h!]
	\centering
	\hspace*{-0.5cm}\includegraphics[scale=0.55]{img/cuda_data_blocks}
	\caption{Przejœcie z u³o¿enia w¹tków w bloku na bloki danych}
	\label{ref:cuda_data_blocks}
\end{figure}

Ka¿dej grupie w bloku CUDA odpowiada tyle samo bloków danych, przy czym bloki danych s¹ wielkoœci $\kappa$ elementów i ich numery s¹ potrzebne do obliczenia pocz¹tkowego indeksu wejœciowego dla w¹tku, który definiujemy jako $$ \mu_{0}(t) = d_{b}(t)*\kappa + W_{lane}(t) $$
Kolejne indeksy wyliczane s¹ jako przesuniêcia o $\omega$, czyli 
$ \mu_{k}(t) = \mu_{0}(t) + k * \omega $, dla $ k \in [1,\kappa-1] $. 
Tworzy to podzia³ danych wejœciowych, który zgodnie z przyk³adem obrazuje rysunek \ref{ref:cuda_data_blocks2}. 

\begin{figure}[h!]
	\centering
	\hspace*{-0.5cm}\includegraphics[scale=0.55]{img/cuda_data_blocks2}
	\caption{Indeksy danych wejœciowych}
	\label{ref:cuda_data_blocks2}
\end{figure}

Dziêki takiemu u³o¿eniu kolejne w¹tki z tego samego warp, wykonuj¹ odczyt kolejnych rekordów z tablicy Ÿród³owej, poniewa¿ w¹tek 0 odczytuje rekord 0, w¹tek 1 rekord 1, a¿ wreszcie w¹tek $\omega$ rekord $\omega$, nastêpnie po przesuniêciu o $\omega$ w¹tki ponownie odczytuj¹ dane, które s¹ ze sob¹ s¹siaduj¹ce. Dziêki temu odczyt mo¿e byæ po³¹czony (Global Memory Coalescing\cite{UENG}). £¹czny odczyt mo¿e byæ wielokrotnie szybszy, ni¿ odczyty z nie kolejnych indeksów, co jest kluczowe przy budowie bardzo wydajnych algorytmów przetwarzaj¹cych dane na GPU.
W przypadku CUDA grupa powinna mieæ licznoœæ 32, co odpowiada iloœci w¹tków w jednym \emph{warp}.

\newpage

\subsection{Algorytmy kompresji szeregów}
\begin{description}
\item[RLE] -- kodowanie d³ugoœci serii (Run-Length-Encoding) to sposób kompresji polegaj¹cy na zapisie ci¹gów takich samych wartoœci, jako wartoœæ i d³ugoœæ tego ci¹gu. W tym przypadku obie te wartoœci trafiaj¹ do 2 ró¿nych tablic, które nastêpnie mog¹ byæ kompresowane osobno. Szczególnie dobrze sprawdza siê w przypadku posortowanych danych, lub czêsto siê powtarzaj¹cych. Dla przyk³adu wektor ${5,5,5,5,1,1,1,1,17,17,17,17}$ zostanie skompresowany do ${5,1,17}$ oraz ${4,4,4}$. Data technika kompresji jest op³acalna jeœli œrednia d³ugoœæ ci¹gów przekracza 2 ($D_{sr} > 2$). \\ Dana metoda zosta³a zaimplementowana z u¿yciem biblioteki Thrust, stosuj¹c miêdzy innymi metodê \emph{reduce\_by\_key}.
\item[Dict] -- Kolejn¹ grup¹ kompresji s¹ kompresje bazuj¹ce na pomyœle s³ownikowym ($Dict_{K}$). Wykorzystuj¹ one informacjê o licznoœci poszczególnych wartoœci w wektorze, tj. które wartoœci najczêœciej siê powtarzaj¹. Kompresja s³ownikowa wykorzystuje $K$ najczêœciej wystêpuj¹cych wartoœci i zapisuje ich tablicê w metadanych. Nastêpnie wartoœci z wektora danych s¹ kodowane indeksami w tej tablicy, przy u¿yciu jak najmniejszej liczby bitów $bit_{cnt} = log_{2}(K)$. Reszta niepasuj¹cych wartoœci zapisywana jest do osobnej tablicy, co dzia³a podobnie jak w kodowaniu \emph{Patch}.
\item[Unique, Const] -- Zaimplementowane s¹ tak¿e lekko zoptymalizowane wersje kodowania $Dict_{K}$ dla $K=1$ - \emph{Const}, oraz $K=N_{u}$ - \emph{Unique}, gdzie $N_{u}$ jest liczb¹ unikalnych wartoœci w ca³ym kompresowanym wektorze. Dla przyk³adu jeœli wszystkie wartoœci s¹ równe, z nielicznymi wyj¹tkami, zostanie wybrane kodowanie \emph{Const}, które zapisze najczêstsz¹ wartoœæ w metadanych i stworzy tablicê wyj¹tków, uzyskuj¹c bardzo wysoki stopieñ kompresji.
\end{description}

\subsubsection{AFL}
Ten rodzaj kodowania nazywany jest kodowaniem o sta³ej d³ugoœci z wyrównaniem. Ogólny pomys³ algorytmu jest bardzo prosty i bazuje na algorytmie NS (null suppression), czyli usuwaniu wiod¹cych zer z liczb i zapisywanie ich na mniejszej iloœci bitów. W tym przypadku iloœæ bitów na których zapisujemy liczby jest znana z góry przed rozpoczêciem kodowania i nie ulega zmianie. Dla uproszczenia przyjmijmy, ¿e kompresujemy liczby naturalne o d³ugoœci 32 bitów, np. \emph{unsigned int}. Wyrównanie polega na grupowaniu wykonawczych w¹tków w grupy o pewnej licznoœci. W naszym przypadku bêdzie to liczba 32, czyli liczba w¹tków nale¿¹cych do jednego \emph{warpa}, z uwagi na wykorzystanie tzw. \emph{³¹cznego dostêpu do pamiêci globalnej CUDA}. Wtedy liczba kompresowanych elementów musi byæ wielokrotnoœci¹ d³ugoœci kodowanego s³owa pomno¿onej przez 32, czyli 1024. Kodowanie przebiega nastêpuj¹co:
\begin{enumerate}
\item Obliczana jest najmniejsza liczba bitów potrzebna do zakodowania wszystkich s³ów w wektorze, nazwijmy go $W$ i oznaczmy liczbê bitów jako $\sigma$:
$$ \sigma = log_{2}(max(W)) + 1 $$
\item Nastêpnie wektor elementów jest dope³niany, aby jego d³ugoœæ by³a wielokrotnoœci¹ 1024, a liczba dope³nionych elementów wraz z potrzebn¹ do zapisania liczb liczb¹ bitów $\sigma$ zapisywana jest do metadanych.
\item Dla ka¿dego w¹tku $t$ CUDA ozn. \emph{$t=thread(b_{idx},t_{idx})$}, co oznacza w¹tek o indeksie $t_{idx}$ z bloku $b_{idx}$, wyznaczany jest pocz¹tkowy indeks danych wejœciowych i wyjœciowych dla tego w¹tku, zgodnie z algorytmem przedstawionym w poprzedniej sekcji. 
Wielkoœæ bloków w¹tków ustalmy na $ B_{size} = 32 * 8 $ co sprawi, ¿e ka¿dy blok bêdzie siê sk³ada³ z 8 warpów, a grupy bêd¹ mia³y licznoœæ 32 ($\omega = 32$, $w_{g} = 8$). Ponadto chcemy aby bloki danych by³y wielkoœci 32 elementów ($\kappa = 32$).
Maj¹c dane wejœciowe o indeksach $ {\mu_{0}, \mu_{1}, ..., \mu_{\kappa-1}} $ w¹tek zapisuje $\sigma$ dolnych bitów ka¿dej z liczb spod tych indeksów, do rekordów tablicy wynikowej. Wielkoœci bloków s¹ tak dobrane, aby ka¿dy w¹tek kodowa³ $\kappa$ liczb u¿ywaj¹c $\sigma$ bitów i otrzymuj¹c w ten sposób $\sigma$ liczb o wielkoœci $\kappa$. Za³ó¿my, ¿e $\kappa = 8$ oraz $\sigma = 3$, wtedy \ref{ref:afl4} obrazuje u³o¿enie danych wejœciowych o wielkoœci 8 bitów, skompresowanych do wielkoœci 3 bitów w wektorze wynikowym, przyjmuj¹c 16 iloœæ w¹tków w grupie ($\omega = 16$). Jak widaæ skompresowane dane idealnie mieszcz¹ siê w 3 rekordach tablicy wynikowej.

\begin{figure}[h!]
	\centering
	\hspace*{-0.5cm}\includegraphics[scale=0.55]{img/AFL4}
	\caption{AFL - zapis w tablicy wyjœciowej}
	\label{ref:afl4}
\end{figure}

Pocz¹tkowy indeks tablicy wynikowej pod który w¹tek ma zapisaæ skompresowane dane, wyliczane s¹ jako $ \nu_{0}(t) = d_{b}(t) * \sigma + W_{lane}(t) $. Do rekordu zapisywanych jest mo¿liwie najwiêcej bitów (tak jak pokazuje \ref{ref:afl4}), po czym jeœli brak³o miejsca dalsze bity przenoszone s¹ do nastêpnego rekordu, którego indeks wyliczany jest jako $ \nu_{k}(t) = \nu_{0}(t) + k * \omega $, gdzie $ k \in [1,\sigma-1] $. Tutaj równie¿ w¹tki pos³uguj¹ siê ³¹cznym dostêpem do pamiêci.
\item Po skompresowaniu, dane powinny mieæ dok³adnie $\sigma * \Sigma$ bitów d³ugoœci, nie licz¹c metadanych które mo¿na zapisaæ w dwóch bajtach. 

\end{enumerate}

Tak zaimplementowany algorytm okazuje siê byæ oko³o dziesiêciokrotnie szybszy od tradycyjnej implementacji Fixed-Length encoding na CUDA, który jest równowa¿ny tej kompresji z $\omega = 1$. 

\subsubsection{GFC}
GFC to zaproponowana przez Burtscher et al.\cite{ONEIL} modyfikacja algorytmu pFPC kompresji liczb zmiennoprzecinkowych o podwójnej precyzji, który jest równoleg³¹ wersj¹ algorytmu FPC zaimplementowan¹ na procesory graficzne. Zamiast operacji XOR na prawdziwej i przewidzianej wartoœci kompresowanej liczby, metoda ta u¿ywa zwyk³ego odejmowania i dodatkowo zapamiêtuje znak, a kompresuje wartoœæ absolutn¹ tej ró¿nicy. Algorytm ten równie¿ jest wyrównany u¿ywaj¹c ³¹czny dostêp do pamiêci, za to w przeciwieñstwie do algorytmu AFL nie wymaga aby iloœæ kompresowanych liczb przez pojedynczy w¹tek by³a podzielna przez 32. Mo¿e byæ to na przyk³ad 15. Blok danych bêdzie mia³ ponownie 32 wartoœci, poniewa¿ ka¿dy \emph{warp} musi przetworzyæ 32 wartoœci, aby uzyskaæ $\sigma$ liczb wynikowych. W tym algorytmie liczba $\sigma$ jest wyznaczana na bie¿¹co dla ka¿dej liczby osobno i wyra¿ona jest w bajtach. Opiszê tutaj wersjê algorytmu dla liczb o pojedynczej precyzji (32-bit) typu np. float. Zatem $\sigma$ mo¿e przybieraæ wartoœci 1, 2, 3 lub 4 i mo¿na zapisaæ j¹ na 2 bajtach. Iloœæ bajtów na których zostanie zapisana liczba x mo¿e byæ wyra¿ona w uproszczeniu jako: $ \sigma(x) = log_{2}(x)/8 + 1 $. Poni¿ej zamieszczam prosty pseudokod algorytmu wykonywanego przez ka¿dy w¹tek w kernelu CUDA:
\begin{algorithm}[H]
 \caption{Pseudokod algorytmu kompresji GFC dla w¹tku t}
 \tcc{WEJŒCIE: $data$ - wektor do skompresowania}
 \tcc{WYJŒCIE: $compr$ - skompr. dane, $offsets$ - rozm. paczek}
 $last$ = 0, $i$ = 0\;
 \While{$i < \lambda$}{ 
  $diff$ = $data[\mu_{i}(t)]$ - $last$\;
  $sign$ = bit znaku liczby $diff$\;
  $diff = abs(diff)$\;
  $minByte = \sigma(diff)$\;
  $size$ = wykonaj sumê prefixow¹ na minByte w warpie\;
  $save(compr, diff, minByte)$ \tcp*{zapisz $minByte$ bajtów liczby $diff$ do tablicy wynikowej}
  $saveMeta(compr, minByte, sign)$ \tcp*{zapisz minByte oraz sign do tablicy wynikowej}
  $off = off + size + 16$\;
  $beg = beg + 32$\; 
  $last = data[beg-1]$\;
 }	
 \If{$warp_{idx} == 31$}{$offsets[warp] = off$\;}
 \label{ref:alg_gfc}
\end{algorithm}
\newpage
\noindent
Poszczególne \emph{warp-y} pracuj¹ oddzielnie, kompresuj¹c dane do osobnych paczek. Dane z ca³ego \emph{warp} pakowane s¹ w jedno miejsce, pocz¹wszy od wyliczonego wczeœniej przesuniêcia. W ka¿dej iteracji, \emph{warp} pakuje 32 liczby tworz¹c z nich podpaczkê, poprzedzon¹ ma³ym nag³ówkiem zawieraj¹cym informacje o znakach i liczbie bitów na których jest zapisana ka¿da liczba. Rozmiar tych informacji to 4 bity, wiêc dane w pesymistycznym wypadku mog¹ rozrosn¹æ siê o $l_{w}/2$ bajtów. Podpaczki zapisywane s¹ jedna za drug¹. Poniewa¿ rozmiar skompresowanej paczki nie jest znany w momencie uruchomienia algorytmu, potrzebna jest osobna tablica wynikowa, w której przechowywane s¹ wynikowe wielkoœci paczek. Ponadto trzeba zaalokowaæ tablicê wynikow¹ o wielkoœci
$ c_{s} = (l_{W} + 1)*\frac{9}{2} $ bajtów miejsca. 

\begin{figure}[h!]
	\centering
	\hspace*{-0.5cm}\includegraphics[scale=0.75]{img/gfc}
	\caption{wynik dzia³ania algorytmu GFC - tablica wyjœciowa}
	\label{ref:gfc}
\end{figure}

Kompresja znacz¹co ró¿ni siê od AFL pomimo podobnego podzia³u na bloki danych. Iloœæ stworzonych paczek danych przez ten algorytm wynosi - $ p_{n} = w_{g} * B_{n} $. Paczki skompresowane przez poszczególne \emph{warp-y} nie przylegaj¹ do siebie, co widaæ na rysunku \ref{ref:gfc}, przez to minusem tej metody jest to, ¿e po kompresji nale¿y przekopiowaæ wszystkie $p_{n}$ paczek, o ró¿nych rozmiarach do tablicy wynikowej. Kopiowanie to jest odpowiednio szybkie dla ma³ej iloœci paczek o wiêkszych rozmiarach. Wtedy zachodzi zale¿noœæ, ¿e kiedy $p_{n}$ roœnie wydajnoœæ kopiowania maleje, natomiast wydajnoœæ samego algorytmu wzrasta. Jeœli zmniejszymy $p_{n}$ automatycznie musimy zwiêkszyæ $\lambda$, bo $B_{n}$ zale¿y odwrotnie proporcjonalnie od $\lambda$, a im wiêksze $\lambda$, tym wolniejszy jest sam algorytm (pojedyncze w¹tki wykonuj¹ wiêcej pracy).
Warto sprawdziæ zatem, czy napisanie dedykowanego algorytmu kopiowania (jako kernel CUDA), mo¿e przyspieszyæ takie kopiowania danych na GPU, wzglêdem tradycyjnych wywo³añ \emph{cudaMemcpy} w pêtli. Próby zrównoleglenia tych kopiowañ na ró¿nych \emph{stream-ach} na mojej karcie graficznej (GeForce GT 640) skoñczy³y siê gorsz¹ wydajnoœci¹ ni¿ seria kopiowañ na \emph{stream 0}.


\section{Biblioteki}

Projekt sk³ada siê z 4 bibliotek, których zale¿noœci wewnêtrzne pokazane s¹ na rysunku \ref{ref:libs}. Ponadto biblioteki \emph{CORE} oraz \emph{ENC} wykorzystuj¹ technologiê CUDA i musz¹ byæ kompilowane za pomoc¹ \emph{nvcc}. Wszystkie biblioteki korzystaj¹ z biblioteki \emph{Boost} oraz bibliotek do testowania i benchmarków (\emph{GTEST}, \emph{Google Benchmark}). 
Poni¿ej znajduje siê tak¿e opis poszczególnych bibliotek. 

	\begin{figure}[h!]
		\centering
		\hspace*{-0.5cm}\includegraphics[scale=0.55]{img/Libs}
		\caption{Biblioteki}
		\label{ref:libs}
	\end{figure}

\subsection{TS}
Jest to biblioteka definiuj¹ca strukturê szeregu czasowego, oraz udostêpniaj¹ca metody do odczytu i zapisu szeregów z plików binarnych oraz tekstowych o kolumnach rozdzielonych separatorem, na przyk³ad CSV. Ponadto umo¿liwia definicjê szeregu czasowego poprzez plik nag³ówkowy o strukturze wierszy: ([nazwa kolumny],[typ kolumny],[precyzja]), w której ka¿dy wiersz definiuje osobn¹ kolumnê. Przyk³ad:
\begin{lstlisting}[caption=Przyk³adowy plik opisu szeregu czasowego,label={lst:tsheader}]
		timestamp,time,0
		CORE VOLTAGE,float,6
		CPU TEMP,float,6
		GPU TEMP,float,6
\end{lstlisting}
\subsection{CORE}
Tutaj zaimplementowane s¹ wszystkie podstawowe rzeczy takie jak logowanie, konfiguracja, inteligentny wskaŸnik na pamiêæ CUDA (zaimplementowany w oparciu o \emph{Shared Pointer} z biblioteki \emph{Boost}), operacje na wektorach danych po stronie GPU takie jak histogramy, wyliczanie statystyk itp., a tak¿e bazowe klasy testów i benchmarków wraz z generatorem danych.
\subsection{ENC}
Encodings to biblioteka mieszcz¹ce wszystkie zaimplementowane w ramach tego projektu algorytmy kodowania i transformacji zaimplementowane na CUDA, potrafi¹ce kodowaæ i dekodowaæ dane wszystkich typów obs³ugiwanych przez ten system (\emph{char, short, int, unsigned int, long, float, double}).
\subsection{OPT}
W³aœciwa biblioteka dla tego projektu mieszcz¹ca optymalizator kompresji, a tak¿e definicjê i implementacjê drzewa kompresji, jak równie¿ drzewa optymalnego (drzewa aktualnie u¿ywanego przez kompresor, które umo¿liwia pewne mutowanie tego drzewa, co zostanie opisane w nastêpnym rozdziale).
\section{Program}
Program wynikowy jest przyk³adowym programem wynikowym, który u¿ywaj¹c optymalizatora kompresji, wielow¹tkowo i równolegle kompresuje wiele kolumn szeregu czasowego podanego jako plik wejœciowy. Plik wejœciowy jest zaczytywany paczkami i w tym samym czasie czêœci ju¿ skompresowane mog¹ byæ zapisywane do podanego pliku wyjœciowego. Dok³adny opis tego algorytmu znajduje siê na koñcu rozdzia³u 3. Argumenty programu:
\begin{itemize}
\item \emph{--compress} lub \emph{-c}: opcja kompresji (nast¹pi kompresja pliku wejœciowego)
\item \emph{--decompress} lub \emph{-d}: opcja dekompresji (nast¹pi dekompresja pliku wejœciowego)
\item \emph{--header} lub \emph{-h} \emph{[<œcie¿ka>]}: podanie œcie¿ki do pliku zawieraj¹cego opis szeregu jak wy¿ej \ref{lst:tsheader}.
\item \emph{--input} lub \emph{-i} \emph{[<œcie¿ka>]}: podanie œcie¿ki do pliku wejœciowego
\item \emph{--output} lub \emph{-o} \emph{[<œcie¿ka>]}: podanie œcie¿ki do pliku wyjœciowego
\item \emph{--generate} lub \emph{-g}: -- wygeneruj przyk³adowe pliki danych
\item \emph{--padding} lub \emph{-p} \emph{[<ró¿nica>]}: w przypadku gdy wiersze szeregu w pliku binarnym s¹ wyrównane do jakieœ wielkoœci, podajemy w ten sposób ró¿nicê wzglêdem ich rzeczywistej wielkoœci, np. jeœli dane zajmuj¹ 12 bajtów a s¹ wyrównane do 16, powinniœmy uruchomiæ program z opcj¹ \emph{-p 4}.
\end{itemize}

\chapter{Optymalizator kompresji}

\chapter{Wyniki}

\chapter{Podsumowanie}


%-----------Koniec czêœci zasadniczej-----------

\begin{thebibliography}{11}

\bibitem[1]{PPS} Mark Harris, Shubhabrata Sengupta, and John D. Owens. \emph{"Parallel Prefix Sum (Scan) with CUDA"}. In Hubert Nguyen, editor, GPU Gems 3, chapter 39, pages 851–876. Addison Wesley, August 2007
\bibitem[2]{SORT} Nadathur Satish, Mark Harris, and Michael Garland. \emph{"Designing Efficient Sorting Algorithms for Manycore GPUs"}. In Proceedings of the 23rd IEEE International Parallel \& Distributed Processing Symposium, May 2009
\bibitem[3]{PHASH} Alcantara, Dan A., et al. \emph{"Real-time parallel hashing on the GPU."} ACM Transactions on Graphics (TOG) 28.5 (2009): 154
\bibitem[4]{MOSTAK} Mostak, T., 2013. \emph{"An overview of MapD (massively parallel database)."}, Massachusetts Institute of Technology, Cambridge, MA.
\bibitem[5]{CURRY} Cloud RL, Curry ML, Ward HL, Skjellum A, Bangalore P. \emph{"Accelerating lossless data compression with GPUs."} arXiv preprint arXiv:1107.1525. 2011 Jun 21.
\bibitem[6]{RUSSEK} Pietroñ, M., Pawel Russek, and Kazimierz Wiatr. \emph{"Accelerating SELECT WHERE and SELECT JOIN queries on a GPU."} Computer Science 14.2) (2013): 243-252.
\bibitem[7]{DTU} Nicolaisen, Anders Lehrmann Vr?nning. \emph{"Algorithms for Compression on GPUs."} (2013).
\bibitem[8]{MENSMANN} Mensmann, Jörg, Timo Ropinski, and Klaus Hinrichs. \emph{"A GPU-supported lossless compression scheme for rendering time-varying volume data."} 8th IEEE/EG international conference on Volume Graphics, 2–3 May 2010, Norrköping, Sweden. IEEE, 2010.
\bibitem[9]{BURTSCHER} Burtscher M, Ratanaworabhan P. \emph{"FPC: A high-speed compressor for double-precision floating-point data."} Computers, IEEE Transactions on. 2009 Jan;58(1):18-31.
\bibitem[10]{BAKKUM} Bakkum, Peter, and Kevin Skadron. \emph{"Accelerating SQL database operations on a GPU with CUDA."} Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units. ACM, 2010.
\bibitem[11]{FERRIERA} Ferreira, Miguel C. \emph{"Compression and query execution within column oriented databases."} Diss. Massachusetts Institute of Technology, 2005.
\bibitem[12]{FINK} Fink, Eugene, and Harith Suman Gandhi. \emph{"Compression of time series by extracting major extrema."} Journal of Experimental \& Theoretical Artificial Intelligence 23.2 (2011): 255-270.
\bibitem[13]{KACZMAR2} Przymus, Piotr, and Krzysztof Kaczmarski. \emph{"Compression Planner for Time Series Database with GPU Support."} Transactions on Large-Scale Data-and Knowledge-Centered Systems XV. Springer Berlin Heidelberg, 2014. 36-63.
\bibitem[14]{OZSOY} Ozsoy, Adnan, Martin Swany, and Anamika Chauhan. \emph{"Pipelined parallel lzss for streaming data compression on GPGPUs."} Parallel and Distributed Systems (ICPADS), 2012 IEEE 18th International Conference on. IEEE, 2012.
\bibitem[15]{FANG} Fang, Wenbin, Bingsheng He, and Qiong Luo. \emph{"Database compression on graphics processors."} Proceedings of the VLDB Endowment 3.1-2 (2010): 670-680.
\bibitem[16]{LEMIRE} Lemire, Daniel, and Leonid Boytsov. \emph{"Decoding billions of integers per second through vectorization."} Software: Practice and Experience 45.1 (2015): 1-29.
\bibitem[17]{PRZYMUS1} Przymus, Piotr, and Krzysztof Kaczmarski. \emph{"Dynamic compression strategy for time series database using GPU."} New Trends in Databases and Information Systems. Springer International Publishing, 2014. 235-244.
\bibitem[18]{MANI} Mani, Ganapathy. \emph{"Data Compression using CUDA programming in GPU."} (2012).
\bibitem[19]{CHASH} Alcantara, Dan A., et al. \emph{"Real-time parallel hashing on the GPU."} ACM Transactions on Graphics (TOG) 28.5 (2009): 154.
\bibitem[20]{ONEIL} O'Neil, Molly A., and Martin Burtscher. \emph{"Floating-point data compression at 75 Gb/s on a GPU."} Proceedings of the Fourth Workshop on General Purpose Processing on Graphics Processing Units. ACM, 2011.
\bibitem[21]{ZU} Zu, Yuan, and Bei Hua. \emph{"GLZSS: LZSS lossless data compression can be faster."} Proceedings of Workshop on General Purpose Processing Using GPUs. ACM, 2014.
\bibitem[22]{GPUFS} Silberstein, Mark, et al. \emph{"GPUfs: Integrating a file system with GPUs."} ACM Transactions on Computer Systems (TOCS) 32.1 (2014): 1.
\bibitem[23]{ACC} Al-Kiswany, Samer, Ammar Gharaibeh, and Matei Ripeanu. \emph{"GPUs as storage system accelerators."} Parallel and Distributed Systems, IEEE Transactions on 24.8 (2013): 1556-1566.
\bibitem[23]{PRZYMUS2} Przymus, Piotr, and Krzysztof Kaczmarski. \emph{"Improving efficiency of data intensive applications on GPU using lightweight compression."} On the Move to Meaningful Internet Systems: OTM 2012 Workshops. Springer Berlin Heidelberg, 2012.
\bibitem[24]{ABADI} Abadi, Daniel, Samuel Madden, and Miguel Ferreira. \emph{"Integrating compression and execution in column-oriented database systems."} Proceedings of the 2006 ACM SIGMOD international conference on Management of data. ACM, 2006.
\bibitem[25]{ANH} Anh, Vo Ngoc, and Alistair Moffat. \emph{"Inverted index compression using word-aligned binary codes."} Information Retrieval 8.1 (2005): 151-166.
\bibitem[26]{EIROLA} Eirola, Axel. \emph{"Lossless data compression on GPGPU architectures."} arXiv preprint arXiv:1109.2348 (2011).
\bibitem[27]{SHYNI} Shyni, K., and Manoj Kumar KV. \emph{"Lossless LZW Data Compression Algorithm on CUDA."} IOSR Journal of Computer Engineering (IOSR-JCE) 13.1 (2013): 122-127.
\bibitem[28]{MAPD} Mostak, Todd. \emph{"An overview of MapD (massively parallel database)."} White paper, Massachusetts Institute of Technology, Cambridge, MA (2013).
\bibitem[29]{BUSCHS} Buchsbaum, Adam L., et al. \emph{"Engineering the compression of massive tables: an experimental approach."} Symposium on Discrete Algorithms: Proceedings of the eleventh annual ACM-SIAM symposium on Discrete algorithms. Vol. 9. No. 11. 2000.
\bibitem[30]{MORISHIMA} Morishima, Shin, and Hiroki Matsutani. \emph{"Performance Evaluations of Document-Oriented Databases Using GPU and Cache Structure."} Trustcom/BigDataSE/ISPA, 2015 IEEE. Vol. 3. IEEE, 2015.
\bibitem[31]{OZSOY3} Ozsoy, Adnan, Martin Swany, and Arun Chauhan. \emph{"Optimizing LZSS compression on GPGPUs."} Future Generation Computer Systems 30 (2014): 170-178.
\bibitem[32]{PATEL} Patel, Ritesh A., et al. \emph{"Parallel lossless data compression on the GPU."} IEEE, 2012.
\bibitem[33]{SIMD} Polychroniou, Orestis, Arun Raghavan, and Kenneth A. Ross. \emph{"Rethinking SIMD vectorization for in-memory databases."} Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data. ACM, 2015.
\bibitem[34]{SHREDDER} Bhatotia, Pramod, Rodrigo Rodrigues, and Akshat Verma. \emph{"Shredder: GPU-accelerated incremental storage and computation."} FAST. 2012.
\bibitem[35]{ZUKOWSKI} Zukowski, Marcin, et al. \emph{"Super-scalar RAM-CPU cache compression."} Data Engineering, 2006. ICDE'06. Proceedings of the 22nd International Conference on. IEEE, 2006.
\bibitem[36]{EICHINGER} Eichinger, Frank, et al. \emph{"A time-series compression technique and its application to the smart grid."} The VLDB Journal 24.2 (2015): 193-218.
\bibitem[37]{KACZMAR1} Przymus, Piotr, and Krzysztof Kaczmarski. \emph{"Time series queries processing with gpu support."} New Trends in Databases and Information Systems. Springer International Publishing, 2014. 53-60.
\bibitem[38]{ZHAO} Zhao, Wayne Xin, et al. \emph{"A General SIMD-based Approach to Accelerating Compression Algorithms."} ACM Transactions on Information Systems (TOIS) 33.3 (2015): 15.
\bibitem[39]{GOLDSTEIN} Goldstein, Jonathan, Raghu Ramakrishnan, and Uri Shaft. \emph{"Compressing relations and indexes."} Data Engineering, 1998. Proceedings., 14th International Conference on. IEEE, 1998.
\bibitem[41]{CUDA_PERF} http://developer.download.nvidia.com/compute/cuda/compute-docs/cuda-performance-report.pdf
\bibitem[42]{PARETO} Marler, R. Timothy, and Jasbir S. Arora. \emph{"Survey of multi-objective optimization methods for engineering."} Structural and multidisciplinary optimization 26.6 (2004): 369-395.
\bibitem[43]{UENG} Ueng, Sain-Zee, et al. \emph{"CUDA-lite: Reducing GPU programming complexity."} Languages and Compilers for Parallel Computing. Springer Berlin Heidelberg, 2008. 1-15.
\bibitem[44]{FPC} Burtscher, Martin, and Paruj Ratanaworabhan. \emph{"pFPC: A parallel compressor for floating-point data."} Data Compression Conference, 2009. DCC'09.. IEEE, 2009.

\end{thebibliography}
\tableofcontents
\clearpage
\pagestyle{empty}
\noindent Warszawa, dnia ...............
\vspace{5cm}
\begin{center}
\LARGE{Oœwiadczenie}
\end{center}
Oœwiadczam, ¿e pracê magistersk¹ pod tytu³em: ,,Tytu³ pracy'', której promotorem jest prof. dr hab. Jan Wybitny, wykona³em/am samodzielnie, co poœwiadczam w³asnorêcznym podpisem.
\vspace{2cm}
\begin{flushright}
...........................................
\end{flushright}

\end{document}